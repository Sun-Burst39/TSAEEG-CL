{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers            \n",
    "from tensorflow.keras.layers import AveragePooling2D,Dense,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers,optimizers,losses\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.layers import Input,Flatten,LSTM\n",
    "from tensorflow.keras.layers import Embedding, Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置GPU使用方式\n",
    "# 获取GPU列表\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 设置GPU为增长式占用\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except RuntimeError as e:\n",
    "        #打印异常\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 16\n",
    "data_all=np.load('F:/An/脑电/数据处理/epoch/训练改/xunlian.npy')\n",
    "label_all=np.concatenate((np.zeros((4549,1),dtype=int),np.ones((4788,1),dtype=int)),axis=0)\n",
    "label_all=np.eye(2)[label_all]\n",
    "label_all=np.squeeze(label_all)\n",
    "n=len(data_all)\n",
    "A = np.linspace(0,n-1,n,dtype=int)\n",
    "random.shuffle(A)\n",
    "data_all=data_all[A]\n",
    "label_all=label_all[A]\n",
    "kfold=KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "historys,test_pred,test_real,accuracy,precision,recall,f1score=list(),list(),list(),list(),list(),list(),list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbeddingLayer(Layer):\n",
    "    def __init__(self, num_channels, sequence_length, output_dim, **kwargs):\n",
    "        super(TimeEmbeddingLayer, self).__init__(**kwargs)\n",
    "        self.num_channels = num_channels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "        self.time_embedding_layer = Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position = tf.range(self.sequence_length)\n",
    "        embed = self.time_embedding_layer(position)\n",
    "        embed = tf.reshape(embed, (1, 1, self.sequence_length, self.output_dim))\n",
    "        return inputs + embed\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(TimeEmbeddingLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"num_channels\": self.num_channels,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class ChannelEmbeddingLayer(Layer):\n",
    "    def __init__(self, num_channels, sequence_length, output_dim, **kwargs):\n",
    "        super(ChannelEmbeddingLayer, self).__init__(**kwargs)\n",
    "        self.num_channels = num_channels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "        self.channel_embedding_layer = Embedding(input_dim=num_channels, output_dim=output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        position = tf.range(self.num_channels)\n",
    "        embed = self.channel_embedding_layer(position)\n",
    "        embed = tf.reshape(embed, (1, self.num_channels, 1, self.output_dim))\n",
    "        return inputs + embed\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ChannelEmbeddingLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"num_channels\": self.num_channels,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def mymodel1():\n",
    "    input = layers.Input(shape=(30,36,40))\n",
    "    x=TimeEmbeddingLayer(num_channels=30, sequence_length=36, output_dim=40)(input)\n",
    "    x=layers.Conv2D(40,kernel_size=(30,1),strides=(1,1))(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.ReLU()(x)\n",
    "    x=layers.Reshape((x.shape[2],x.shape[3]))(x)\n",
    "    attn_input=x\n",
    "    x=layers.MultiHeadAttention(num_heads=8,key_dim=40)(x,x)\n",
    "    x=layers.Add()([x,attn_input])\n",
    "    x=layers.LSTM(units=100, return_sequences=True)(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def mymodel2():\n",
    "    input = layers.Input(shape=(30,36,40))\n",
    "    x=ChannelEmbeddingLayer(num_channels=30, sequence_length=36, output_dim=40)(input)\n",
    "    x=layers.Conv2D(40, kernel_size=(1,36), strides=(1, 1))(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.ReLU()(x)\n",
    "    x=layers.Reshape((x.shape[1],x.shape[3]))(x)\n",
    "    attn_input=x\n",
    "    x=layers.MultiHeadAttention(num_heads=8,key_dim=40)(x,x)\n",
    "    x=layers.Add()([x,attn_input])\n",
    "    x=layers.LSTM(units=100, return_sequences=True)(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def siamese_network(inp_shape=(30,200,1)):\n",
    "    inp=Input(shape=inp_shape)\n",
    "    x=layers.Conv2D(40,kernel_size=(1,15),strides=(1,1),\n",
    "                      name='conv_1d_temporal',\n",
    "                      kernel_regularizer=regularizers.l2(0.01),\n",
    "                      bias_regularizer=regularizers.l2(0.01)\n",
    "                      )(inp)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.ReLU()(x)\n",
    "    x=AveragePooling2D(pool_size=(1,10),strides=(1,5))(x)\n",
    "    out1=mymodel1()(x)\n",
    "    out2=mymodel2()(x)\n",
    "    merged=tf.keras.layers.concatenate([out1,out2],axis=1)\n",
    "    x=layers.Flatten()(merged)\n",
    "    x=layers.Dense(100, activation=\"relu\")(x)\n",
    "    x=layers.Dropout(0.5)(x)\n",
    "    x=layers.Dense(50, activation=\"relu\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    return model\n",
    "\n",
    "def mymodel(dropoutRate=0.5):\n",
    "    inp_shape=(2,30,200,1)\n",
    "    inp = layers.Input(shape=inp_shape)\n",
    "    split1,split2=tf.unstack(inp,axis=1)\n",
    "    siamese=siamese_network(inp_shape=(30, 200, 1))\n",
    "    feature1=siamese(split1)\n",
    "    feature2=siamese(split2)\n",
    "    distance = tf.sqrt(tf.reduce_sum(tf.square(feature1 - feature2), axis=1, keepdims=True))\n",
    "    distance_output = layers.Lambda(lambda x: x, name='distance_output')(distance)\n",
    "    concat_features = layers.Concatenate()([feature1, feature2, distance])\n",
    "    x=layers.Dense(100, activation='relu')(concat_features)\n",
    "    x=layers.Dropout(dropoutRate)(x)\n",
    "    x=layers.Dense(50, activation='relu')(x)\n",
    "    classification_output = layers.Dense(2, activation='softmax', name='classification_output')(x)\n",
    "    model = keras.models.Model(inputs=inp, outputs=[distance_output, classification_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(margin=2.0):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        label = tf.cast(tf.argmax(y_true, axis=1), tf.float32)\n",
    "        same_class_loss = label * tf.square(y_pred)\n",
    "        diff_class_loss = (1 - label) * tf.square(tf.maximum(margin - y_pred, 0))\n",
    "        return tf.reduce_mean(same_class_loss + diff_class_loss)\n",
    "    return loss_fn\n",
    "\n",
    "# 创建十折交叉验证中显示第几折的指示变量\n",
    "ind_fold=0\n",
    "# 进行十折交叉验证\n",
    "for train_ind,test_ind in kfold.split(data_all,label_all):\n",
    "    ind_fold=ind_fold+1\n",
    "    print('fold hao:',ind_fold)\n",
    "    n=len(train_ind)\n",
    "    A=np.linspace(0,n-1,n,dtype=int)\n",
    "    random.shuffle(A)\n",
    "    # 构建训练集、验证集、测试集\n",
    "    epoch_train=data_all[train_ind[A[:int(0.8*n)]]]\n",
    "    epoch_val=data_all[train_ind[A[int(0.8*n):]]]\n",
    "    epoch_test=data_all[test_ind]\n",
    "    label_train=label_all[train_ind[A[:int(0.8*n)]]]\n",
    "    label_val=label_all[train_ind[A[int(0.8*n):]]]\n",
    "    label_test=label_all[test_ind]\n",
    "    print(epoch_train.shape)\n",
    "    print(label_train.shape)\n",
    "    label_train = tf.cast(label_train, tf.float32)\n",
    "    label_val = tf.cast(label_val, tf.float32)\n",
    "    label_test = tf.cast(label_test, tf.float32)\n",
    "\n",
    "    db_train=tf.data.Dataset.from_tensor_slices((\n",
    "        epoch_train,\n",
    "        {\n",
    "            'distance_output': label_train,\n",
    "            'classification_output': label_train\n",
    "        }\n",
    "    ))\n",
    "    db_val=tf.data.Dataset.from_tensor_slices((\n",
    "        epoch_val,\n",
    "        {\n",
    "            'distance_output': label_val,\n",
    "            'classification_output': label_val\n",
    "        }\n",
    "    ))\n",
    "    db_train=db_train.shuffle(1000).batch(batchsz)\n",
    "    db_val=db_val.shuffle(1000).batch(batchsz)\n",
    "    model=mymodel(nb_classes=2,dropoutRate=0.3)  \n",
    "    model.summary()\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_classification_output_accuracy',\n",
    "        min_delta=0.001,\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_classification_output_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "                   loss={\n",
    "                       'distance_output': contrastive_loss(margin=2.0),\n",
    "                       'classification_output': 'categorical_crossentropy'\n",
    "                   },\n",
    "                   loss_weights={\n",
    "                       'distance_output': 0.3,  # 对比损失权重\n",
    "                       'classification_output': 1.0  # 分类损失权重\n",
    "                   },\n",
    "                   metrics={\n",
    "                       'classification_output': 'accuracy'\n",
    "                   })\n",
    "    \n",
    "    print('开始训练!!') \n",
    "    history=model.fit(db_train, \n",
    "                     validation_data=db_val,\n",
    "                     validation_freq=1,  \n",
    "                     shuffle=True, \n",
    "                     epochs=100,\n",
    "                     callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    history = history.history\n",
    "    historys.append(history)\n",
    "    [_, pred_probs] = model.predict(epoch_test)\n",
    "    pred_test = np.argmax(pred_probs, axis=1)\n",
    "    label_test_indices = np.argmax(label_test, axis=1)\n",
    "    test_pred.append(pred_test)\n",
    "    test_real.append(label_test_indices)\n",
    "    \n",
    "    # 计算准确率，精确率，召回率，f1评分\n",
    "    acc=accuracy_score(label_test_indices, pred_test)\n",
    "    pre=precision_score(label_test_indices, pred_test, average='macro')\n",
    "    rec=recall_score(label_test_indices, pred_test, average='macro')\n",
    "    f1=f1_score(label_test_indices, pred_test, average='macro')\n",
    "    accuracy.append(acc)\n",
    "    precision.append(pre)\n",
    "    recall.append(rec)\n",
    "    f1score.append(f1)\n",
    "    print(f\"$$ 测试集准确率为 accuracy:{acc}\")\n",
    "    print(f\"$$ 测试集精确率为 precision:{pre}\")\n",
    "    print(f\"$$ 测试集召回率为 recall:{rec}\")\n",
    "    print(f\"$$ 测试集f1评分为 f1_score:{f1}\")\n",
    "# 将每一折history中误差结果保存（训练集和测试集，用于反映训练过程）    \n",
    "loss_train=[]\n",
    "loss_val=[]\n",
    "for history_s in historys:\n",
    "    loss_val.append(history_s['val_loss'])\n",
    "    loss_train.append(history_s['loss'])\n",
    "\n",
    "print(f\"$$ 测试集准确率为 accuracy:{np.mean(accuracy)}\")\n",
    "print(f\"$$ 测试集精确率为 precision:{np.mean(precision)}\")\n",
    "print(f\"$$ 测试集召回率为 recall:{np.mean(recall)}\")\n",
    "print(f\"$$ 测试集f1评分为 f1_score:{np.mean(f1score)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tens",
   "language": "python",
   "name": "kernelname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
